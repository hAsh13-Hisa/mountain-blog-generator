#!/usr/bin/env python3
"""
BE-PALè¨˜äº‹çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ - å†åˆ©ç”¨å¯èƒ½ãªã‚·ã‚¹ãƒ†ãƒ 
æ–°ã—ã„BE-PALè¨˜äº‹ã‚„å¤–éƒ¨è¨˜äº‹ã‚’å±±ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«çµ±åˆã™ã‚‹ãŸã‚ã®æ¨™æº–åŒ–ã•ã‚ŒãŸãƒ„ãƒ¼ãƒ«
"""
import json
import re
from datetime import datetime
from pathlib import Path

class BePalIntegrationWorkflow:
    def __init__(self):
        self.mountains_db_path = Path("data/mountains_japan_expanded.json")
        self.elevation_limit = 400  # æ¨™é«˜åˆ¶é™
        self.workflow_log = []
    
    def log_step(self, step, message):
        """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é€²è¡Œãƒ­ã‚°"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_entry = f"[{timestamp}] {step}: {message}"
        self.workflow_log.append(log_entry)
        print(f"ğŸ”„ {log_entry}")
    
    def extract_mountain_data_from_text(self, text_content, source_info):
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å±±ã®æƒ…å ±ã‚’æŠ½å‡º
        
        Parameters:
        text_content (str): è¨˜äº‹ã®ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹
        source_info (dict): ã‚½ãƒ¼ã‚¹æƒ…å ±ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã€URLã€å‡ºç‰ˆç¤¾ãªã©ï¼‰
        
        Returns:
        list: æŠ½å‡ºã•ã‚ŒãŸå±±ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        """
        self.log_step("EXTRACT", "è¨˜äº‹ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å±±ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºé–‹å§‹")
        
        extracted_mountains = []
        
        # æ¨™é«˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒãƒƒãƒãƒ³ã‚°ï¼ˆæ¨™é«˜XXXmï¼‰
        elevation_pattern = r'æ¨™é«˜(\d+(?:\.\d+)?)m'
        # å±±åãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒãƒƒãƒãƒ³ã‚°
        mountain_pattern = r'([éƒ½é“åºœçœŒ]{2,3}ï¼‰ï½œ([^ï½œ\n]+)'
        
        lines = text_content.split('\n')
        current_mountain = None
        
        for i, line in enumerate(lines):
            # éƒ½é“åºœçœŒï½œå±±å ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
            if 'ï½œ' in line and any(pref in line for pref in ['çœŒ', 'åºœ', 'é“', 'éƒ½']):
                parts = line.split('ï½œ')
                if len(parts) >= 2:
                    prefecture = parts[0].strip()
                    mountain_name = parts[1].strip()
                    
                    # æ¬¡ã®è¡Œã§æ¨™é«˜ã‚’æ¢ã™
                    next_lines = lines[i+1:i+5]  # æ¬¡ã®4è¡Œã‚’ç¢ºèª
                    elevation = None
                    description = ""
                    
                    for next_line in next_lines:
                        elevation_match = re.search(elevation_pattern, next_line)
                        if elevation_match:
                            elevation = float(elevation_match.group(1))
                            break
                    
                    # æ¨™é«˜åˆ¶é™ãƒã‚§ãƒƒã‚¯
                    if elevation and elevation <= self.elevation_limit:
                        # èª¬æ˜æ–‡ã‚’æŠ½å‡ºï¼ˆæ¨™é«˜è¡Œã‹ã‚‰æ•°è¡Œå…ˆã¾ã§ï¼‰
                        desc_start = i + 2
                        desc_lines = []
                        for j in range(desc_start, min(desc_start + 10, len(lines))):
                            if j < len(lines):
                                desc_line = lines[j].strip()
                                if desc_line and not desc_line.startswith('â–¼') and not desc_line.startswith('image'):
                                    desc_lines.append(desc_line)
                                elif desc_line.startswith('â–¼'):
                                    break
                        
                        description = ' '.join(desc_lines)
                        
                        mountain_data = {
                            "name": mountain_name,
                            "prefecture": prefecture,
                            "elevation": elevation,
                            "description": description,
                            "source": source_info
                        }
                        
                        extracted_mountains.append(mountain_data)
                        self.log_step("FOUND", f"{mountain_name} ({prefecture}, {elevation}m)")
        
        self.log_step("EXTRACT", f"æŠ½å‡ºå®Œäº†: {len(extracted_mountains)}å±±ã‚’ç™ºè¦‹")
        return extracted_mountains
    
    def create_mountain_id(self, mountain_name, prefecture):
        """å±±IDã‚’ç”Ÿæˆ"""
        # å±±åã‹ã‚‰ä¸è¦ãªæ–‡å­—ã‚’é™¤å»ã—ã€ãƒ­ãƒ¼ãƒå­—é¢¨ã®IDã‚’ç”Ÿæˆ
        clean_name = re.sub(r'[å±±å²³å³°å¶½]', '', mountain_name)
        prefecture_short = {
            'åŒ—æµ·é“': 'hokkaido', 'é’æ£®çœŒ': 'aomori', 'å²©æ‰‹çœŒ': 'iwate', 'å®®åŸçœŒ': 'miyagi',
            'ç§‹ç”°çœŒ': 'akita', 'å±±å½¢çœŒ': 'yamagata', 'ç¦å³¶çœŒ': 'fukushima', 'èŒ¨åŸçœŒ': 'ibaraki',
            'æ ƒæœ¨çœŒ': 'tochigi', 'ç¾¤é¦¬çœŒ': 'gunma', 'åŸ¼ç‰çœŒ': 'saitama', 'åƒè‘‰çœŒ': 'chiba',
            'æ±äº¬éƒ½': 'tokyo', 'ç¥å¥ˆå·çœŒ': 'kanagawa', 'æ–°æ½ŸçœŒ': 'niigata', 'å¯Œå±±çœŒ': 'toyama',
            'çŸ³å·çœŒ': 'ishikawa', 'ç¦äº•çœŒ': 'fukui', 'å±±æ¢¨çœŒ': 'yamanashi', 'é•·é‡çœŒ': 'nagano',
            'å²é˜œçœŒ': 'gifu', 'é™å²¡çœŒ': 'shizuoka', 'æ„›çŸ¥çœŒ': 'aichi', 'ä¸‰é‡çœŒ': 'mie',
            'æ»‹è³€çœŒ': 'shiga', 'äº¬éƒ½åºœ': 'kyoto', 'å¤§é˜ªåºœ': 'osaka', 'å…µåº«çœŒ': 'hyogo',
            'å¥ˆè‰¯çœŒ': 'nara', 'å’Œæ­Œå±±çœŒ': 'wakayama', 'é³¥å–çœŒ': 'tottori', 'å³¶æ ¹çœŒ': 'shimane',
            'å²¡å±±çœŒ': 'okayama', 'åºƒå³¶çœŒ': 'hiroshima', 'å±±å£çœŒ': 'yamaguchi', 'å¾³å³¶çœŒ': 'tokushima',
            'é¦™å·çœŒ': 'kagawa', 'æ„›åª›çœŒ': 'ehime', 'é«˜çŸ¥çœŒ': 'kochi', 'ç¦å²¡çœŒ': 'fukuoka',
            'ä½è³€çœŒ': 'saga', 'é•·å´çœŒ': 'nagasaki', 'ç†Šæœ¬çœŒ': 'kumamoto', 'å¤§åˆ†çœŒ': 'oita',
            'å®®å´çœŒ': 'miyazaki', 'é¹¿å…å³¶çœŒ': 'kagoshima', 'æ²–ç¸„çœŒ': 'okinawa'
        }.get(prefecture, 'unknown')
        
        # ç°¡å˜ãªãƒ­ãƒ¼ãƒå­—å¤‰æ›ï¼ˆåŸºæœ¬çš„ãªã‚‚ã®ã®ã¿ï¼‰
        name_romaji = clean_name.lower().replace('ãƒ¶', 'ga').replace('ãƒ¼', '').replace('ãƒ»', '')
        
        return f"mt_{name_romaji}_{prefecture_short}"
    
    def convert_to_full_mountain_data(self, extracted_mountain, existing_mountains):
        """
        æŠ½å‡ºã•ã‚ŒãŸå±±ãƒ‡ãƒ¼ã‚¿ã‚’å®Œå…¨ãªå±±ãƒ‡ãƒ¼ã‚¿å½¢å¼ã«å¤‰æ›
        """
        mountain_id = self.create_mountain_id(extracted_mountain['name'], extracted_mountain['prefecture'])
        
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
        if any(m['id'] == mountain_id for m in existing_mountains):
            self.log_step("SKIP", f"{extracted_mountain['name']} - æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨")
            return None
        
        # åœ°åŸŸåˆ¤å®š
        region_map = {
            'åŒ—æµ·é“': 'åŒ—æµ·é“',
            'é’æ£®çœŒ': 'æ±åŒ—', 'å²©æ‰‹çœŒ': 'æ±åŒ—', 'å®®åŸçœŒ': 'æ±åŒ—', 'ç§‹ç”°çœŒ': 'æ±åŒ—', 'å±±å½¢çœŒ': 'æ±åŒ—', 'ç¦å³¶çœŒ': 'æ±åŒ—',
            'èŒ¨åŸçœŒ': 'é–¢æ±', 'æ ƒæœ¨çœŒ': 'é–¢æ±', 'ç¾¤é¦¬çœŒ': 'é–¢æ±', 'åŸ¼ç‰çœŒ': 'é–¢æ±', 'åƒè‘‰çœŒ': 'é–¢æ±', 'æ±äº¬éƒ½': 'é–¢æ±', 'ç¥å¥ˆå·çœŒ': 'é–¢æ±',
            'æ–°æ½ŸçœŒ': 'ä¸­éƒ¨', 'å¯Œå±±çœŒ': 'ä¸­éƒ¨', 'çŸ³å·çœŒ': 'ä¸­éƒ¨', 'ç¦äº•çœŒ': 'ä¸­éƒ¨', 'å±±æ¢¨çœŒ': 'ä¸­éƒ¨', 'é•·é‡çœŒ': 'ä¸­éƒ¨', 'å²é˜œçœŒ': 'ä¸­éƒ¨', 'é™å²¡çœŒ': 'ä¸­éƒ¨', 'æ„›çŸ¥çœŒ': 'ä¸­éƒ¨',
            'ä¸‰é‡çœŒ': 'é–¢è¥¿', 'æ»‹è³€çœŒ': 'é–¢è¥¿', 'äº¬éƒ½åºœ': 'é–¢è¥¿', 'å¤§é˜ªåºœ': 'é–¢è¥¿', 'å…µåº«çœŒ': 'é–¢è¥¿', 'å¥ˆè‰¯çœŒ': 'é–¢è¥¿', 'å’Œæ­Œå±±çœŒ': 'é–¢è¥¿',
            'é³¥å–çœŒ': 'ä¸­å›½', 'å³¶æ ¹çœŒ': 'ä¸­å›½', 'å²¡å±±çœŒ': 'ä¸­å›½', 'åºƒå³¶çœŒ': 'ä¸­å›½', 'å±±å£çœŒ': 'ä¸­å›½',
            'å¾³å³¶çœŒ': 'å››å›½', 'é¦™å·çœŒ': 'å››å›½', 'æ„›åª›çœŒ': 'å››å›½', 'é«˜çŸ¥çœŒ': 'å››å›½',
            'ç¦å²¡çœŒ': 'ä¹å·', 'ä½è³€çœŒ': 'ä¹å·', 'é•·å´çœŒ': 'ä¹å·', 'ç†Šæœ¬çœŒ': 'ä¹å·', 'å¤§åˆ†çœŒ': 'ä¹å·', 'å®®å´çœŒ': 'ä¹å·', 'é¹¿å…å³¶çœŒ': 'ä¹å·', 'æ²–ç¸„çœŒ': 'ä¹å·'
        }
        
        region = region_map.get(extracted_mountain['prefecture'], 'ä¸æ˜')
        
        # å®Œå…¨ãªå±±ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ä½œæˆ
        full_mountain_data = {
            "id": mountain_id,
            "name": extracted_mountain['name'],
            "name_en": f"Mount {extracted_mountain['name']}",
            "prefecture": extracted_mountain['prefecture'],
            "region": region,
            "elevation": extracted_mountain['elevation'],
            "location": {
                "latitude": 0.0,  # è¦æ‰‹å‹•å…¥åŠ›
                "longitude": 0.0,  # è¦æ‰‹å‹•å…¥åŠ›
                "nearest_station": "è¦èª¿æŸ»",
                "access_time": "è¦èª¿æŸ»"
            },
            "difficulty": {
                "level": "åˆç´š",  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                "hiking_time": "è¦èª¿æŸ»",
                "distance": "è¦èª¿æŸ»",
                "elevation_gain": f"ç´„{int(extracted_mountain['elevation'] * 0.8)}m"
            },
            "features": ["è¦èª¿æŸ»"],
            "seasons": {
                "best": ["æ˜¥", "ç§‹"],
                "cherry_blossom": "è¦èª¿æŸ»",
                "autumn_leaves": "è¦èª¿æŸ»"
            },
            "keywords": [extracted_mountain['prefecture'].replace('çœŒ', '').replace('åºœ', '').replace('é“', '').replace('éƒ½', ''), "ä½å±±", "åˆå¿ƒè€…å‘ã‘"],
            "article_themes": [
                f"{extracted_mountain['name']}ã®é­…åŠ›",
                f"{extracted_mountain['prefecture']}ã®ä½å±±ãƒã‚¤ã‚­ãƒ³ã‚°"
            ],
            "source_reference": {
                "description": extracted_mountain['description'],
                "source": extracted_mountain['source']['title'],
                "source_url": extracted_mountain['source']['url'],
                "extraction_date": datetime.now().strftime("%Y-%m-%d")
            }
        }
        
        self.log_step("CONVERT", f"{extracted_mountain['name']} ãƒ‡ãƒ¼ã‚¿å¤‰æ›å®Œäº†")
        return full_mountain_data
    
    def integrate_new_mountains(self, extracted_mountains):
        """
        æ–°ã—ã„å±±ãƒ‡ãƒ¼ã‚¿ã‚’æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«çµ±åˆ
        """
        self.log_step("INTEGRATE", "æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿")
        
        with open(self.mountains_db_path, 'r', encoding='utf-8') as f:
            mountains_db = json.load(f)
        
        existing_mountains = mountains_db['mountains']
        new_mountains = []
        
        for extracted_mountain in extracted_mountains:
            full_data = self.convert_to_full_mountain_data(extracted_mountain, existing_mountains)
            if full_data:
                new_mountains.append(full_data)
                existing_mountains.append(full_data)
        
        if new_mountains:
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–°
            version_parts = mountains_db['metadata']['version'].split('.')
            new_version = f"{version_parts[0]}.{int(version_parts[1]) + 1}"
            
            mountains_db['metadata']['version'] = new_version
            mountains_db['metadata']['last_updated'] = datetime.now().strftime("%Y-%m-%d")
            mountains_db['metadata']['total_mountains'] = len(existing_mountains)
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜
            with open(self.mountains_db_path, 'w', encoding='utf-8') as f:
                json.dump(mountains_db, f, ensure_ascii=False, indent=2)
            
            self.log_step("SAVE", f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°å®Œäº† (v{new_version})")
            self.log_step("SUCCESS", f"{len(new_mountains)}å±±ã‚’æ–°è¦è¿½åŠ ")
        else:
            self.log_step("INFO", "è¿½åŠ ã§ãã‚‹æ–°ã—ã„å±±ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        
        return new_mountains
    
    def run_full_workflow(self, article_file_path, source_info):
        """
        å®Œå…¨ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œ
        
        Parameters:
        article_file_path (str): è¨˜äº‹ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        source_info (dict): ã‚½ãƒ¼ã‚¹æƒ…å ±
            - title: è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«
            - url: è¨˜äº‹URL
            - publication: å‡ºç‰ˆç¤¾
            - date: å…¬é–‹æ—¥
        
        Returns:
        dict: å®Ÿè¡Œçµæœ
        """
        self.log_step("START", f"BE-PALçµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é–‹å§‹: {article_file_path}")
        
        try:
            # 1. ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            with open(article_file_path, 'r', encoding='utf-8') as f:
                text_content = f.read()
            
            # 2. å±±ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            extracted_mountains = self.extract_mountain_data_from_text(text_content, source_info)
            
            # 3. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆ
            new_mountains = self.integrate_new_mountains(extracted_mountains)
            
            # 4. çµæœã‚µãƒãƒªãƒ¼ä½œæˆ
            summary_file = f"integration_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            summary = {
                "workflow_date": datetime.now().isoformat(),
                "source_info": source_info,
                "extracted_count": len(extracted_mountains),
                "integrated_count": len(new_mountains),
                "new_mountains": new_mountains,
                "workflow_log": self.workflow_log
            }
            
            with open(f"logs/{summary_file}", 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            
            self.log_step("COMPLETE", f"ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Œäº† - ã‚µãƒãƒªãƒ¼: logs/{summary_file}")
            
            return {
                "status": "success",
                "extracted_count": len(extracted_mountains),
                "integrated_count": len(new_mountains),
                "summary_file": summary_file
            }
            
        except Exception as e:
            self.log_step("ERROR", f"ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {
                "status": "error",
                "error": str(e),
                "workflow_log": self.workflow_log
            }

# ä½¿ç”¨ä¾‹ã¨ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
def create_bepal_source_info(article_title, article_url):
    """BE-PALè¨˜äº‹ç”¨ã®ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’ä½œæˆ"""
    return {
        "title": article_title,
        "url": article_url,
        "publication": "BE-PAL",
        "date": datetime.now().strftime("%Y-%m-%d"),
        "copyright_notice": "è¨˜äº‹å†…å®¹ã®å¼•ç”¨æ™‚ã¯å‡ºå…¸ã‚’æ˜è¨˜ã™ã‚‹ã“ã¨"
    }

# CLIå®Ÿè¡Œç”¨ãƒ¡ã‚¤ãƒ³é–¢æ•°
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python bepal_integration_workflow.py <è¨˜äº‹ãƒ•ã‚¡ã‚¤ãƒ«.txt> [è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«] [è¨˜äº‹URL]")
        print()
        print("ä¾‹:")
        print("  python bepal_integration_workflow.py BE-PALè¨˜äº‹.txt")
        print("  python bepal_integration_workflow.py æ–°è¨˜äº‹.txt 'è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«' 'https://example.com'")
        sys.exit(1)
    
    article_file = sys.argv[1]
    article_title = sys.argv[2] if len(sys.argv) > 2 else "è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«æœªè¨­å®š"
    article_url = sys.argv[3] if len(sys.argv) > 3 else "URLæœªè¨­å®š"
    
    # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    Path("logs").mkdir(exist_ok=True)
    
    # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œ
    workflow = BePalIntegrationWorkflow()
    source_info = create_bepal_source_info(article_title, article_url)
    
    result = workflow.run_full_workflow(article_file, source_info)
    
    print("\n" + "="*50)
    print("ğŸ”ï¸ BE-PALçµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çµæœ")
    print("="*50)
    if result["status"] == "success":
        print(f"âœ… æˆåŠŸ")
        print(f"ğŸ“Š æŠ½å‡ºã•ã‚ŒãŸå±±: {result['extracted_count']}å±±")
        print(f"ğŸ†• æ–°è¦è¿½åŠ : {result['integrated_count']}å±±")
        print(f"ğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {result['summary_file']}")
    else:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {result['error']}")
    print("="*50)